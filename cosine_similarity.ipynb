{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multilingual_wordAlignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5STJea0fghEruGD/U8OZt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadaarif-9/huggingface/blob/main/cosine_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimAlign"
      ],
      "metadata": {
        "id": "JczNk9R5uI30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simalign import SentenceAligner"
      ],
      "metadata": {
        "id": "S6-60Hx91ygT"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install simalign\n",
        "#!pip install sacremoses"
      ],
      "metadata": {
        "id": "7YRruFnW1xyZ"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "try:\n",
        "\timport networkx as nx\n",
        "\tfrom networkx.algorithms.bipartite.matrix import from_biadjacency_matrix\n",
        "except ImportError:\n",
        "\tnx = None\n",
        "import torch\n",
        "from transformers import BertModel, BertTokenizer, XLMModel, XLMTokenizer, RobertaModel, RobertaTokenizer, XLMRobertaModel, XLMRobertaTokenizer, AutoConfig, AutoModel, AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "from simalign.utils import get_logger\n",
        "\n",
        "LOG = get_logger(__name__)\n",
        "\n",
        "\n",
        "class EmbeddingLoader(object):\n",
        "\tdef __init__(self, model: str=\"xlm-roberta-base\", device=torch.device('cpu'), layer: int=8):\n",
        "\t\tTR_Models = {\n",
        "\t\t\t'bert-base-uncased': (BertModel, BertTokenizer),\n",
        "\t\t\t'bert-base-multilingual-cased': (BertModel, BertTokenizer),\n",
        "\t\t\t'bert-base-multilingual-uncased': (BertModel, BertTokenizer),\n",
        "\t\t\t'xlm-mlm-100-1280': (XLMModel, XLMTokenizer),\n",
        "\t\t\t'roberta-base': (RobertaModel, RobertaTokenizer),\n",
        "\t\t\t'xlm-roberta-base': (XLMRobertaModel, XLMRobertaTokenizer),\n",
        "\t\t\t'xlm-roberta-large': (XLMRobertaModel, XLMRobertaTokenizer),\n",
        "\t\t}\n",
        "\n",
        "\t\tself.model = model\n",
        "\t\tself.device = device\n",
        "\t\tself.layer = layer\n",
        "\t\tself.emb_model = None\n",
        "\t\tself.tokenizer = None\n",
        "\n",
        "\t\tif False:\n",
        "\t\t\tmodel_class, tokenizer_class = TR_Models[model]\n",
        "\t\t\tself.emb_model = AutoModelForMaskedLM.from_pretrained(model, output_hidden_states=True)\n",
        "\t\t\tself.emb_model.eval()\n",
        "\t\t\tself.emb_model.to(self.device)\n",
        "\t\t\tself.tokenizer = AutoTokenizer.from_pretrained(model,add_prefix_space=True)\n",
        "\t\telse:\n",
        "\t\t\t# try to load model with auto-classes\n",
        "\t\t\tconfig = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
        "\t\t\tself.emb_model = AutoModel.from_pretrained(model, config=config)\n",
        "\t\t\tself.emb_model.eval()\n",
        "\t\t\tself.emb_model.to(self.device)\n",
        "\t\t\tself.tokenizer = AutoTokenizer.from_pretrained(model,add_prefix_space=True) # add_prefix_space=True for roberta and gpt2\n",
        "\t\tLOG.info(\"Initialized the EmbeddingLoader with model: {}\".format(self.model))\n",
        "\n",
        "\tdef get_embed_list(self, sent_batch: List[List[str]]) -> torch.Tensor:\n",
        "\t\tif self.emb_model is not None:\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tif not isinstance(sent_batch[0], str):\n",
        "\t\t\t\t\tinputs = self.tokenizer(sent_batch, is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tinputs = self.tokenizer(sent_batch, is_split_into_words=False, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\t\t\t\thidden = self.emb_model(**inputs.to(self.device))[\"hidden_states\"]\n",
        "\t\t\t\tif self.layer >= len(hidden):\n",
        "\t\t\t\t\traise ValueError(f\"Specified to take embeddings from layer {self.layer}, but model has only {len(hidden)} layers.\")\n",
        "\t\t\t\toutputs = hidden[self.layer]\n",
        "\t\t\t\treturn outputs[:, 1:-1, :]\n",
        "\t\telse:\n",
        "\t\t\treturn None\n",
        "\n",
        "\n",
        "class SentenceAligner(object):\n",
        "\tdef __init__(self, model: str = \"xlm-roberta-base\", token_type: str = \"bpe\", distortion: float = 0.0, matching_methods: str = \"mai\", device: str = \"cpu\", layer: int = 8):\n",
        "\t\tmodel_names = {\n",
        "\t\t\t\"bert\": \"bert-base-multilingual-cased\",\n",
        "\t\t\t\"xlmr\": \"xlm-roberta-base\"\n",
        "\t\t\t}\n",
        "\t\tall_matching_methods = {\"a\": \"inter\", \"m\": \"mwmf\", \"i\": \"itermax\", \"f\": \"fwd\", \"r\": \"rev\"}\n",
        "\n",
        "\t\tself.model = model\n",
        "\t\tif model in model_names:\n",
        "\t\t\tself.model = model_names[model]\n",
        "\t\tself.token_type = token_type\n",
        "\t\tself.distortion = distortion\n",
        "\t\tself.matching_methods = [all_matching_methods[m] for m in matching_methods]\n",
        "\t\tself.device = torch.device(device)\n",
        "\n",
        "\t\tself.embed_loader = EmbeddingLoader(model=self.model, device=self.device, layer=layer)\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef get_max_weight_match(sim: np.ndarray) -> np.ndarray:\n",
        "\t\tif nx is None:\n",
        "\t\t\traise ValueError(\"networkx must be installed to use match algorithm.\")\n",
        "\t\tdef permute(edge):\n",
        "\t\t\tif edge[0] < sim.shape[0]:\n",
        "\t\t\t\treturn edge[0], edge[1] - sim.shape[0]\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn edge[1], edge[0] - sim.shape[0]\n",
        "\t\tG = from_biadjacency_matrix(csr_matrix(sim))\n",
        "\t\tmatching = nx.max_weight_matching(G, maxcardinality=True)\n",
        "\t\tmatching = [permute(x) for x in matching]\n",
        "\t\tmatching = sorted(matching, key=lambda x: x[0])\n",
        "\t\tres_matrix = np.zeros_like(sim)\n",
        "\t\tfor edge in matching:\n",
        "\t\t\tres_matrix[edge[0], edge[1]] = 1\n",
        "\t\treturn res_matrix\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef get_similarity(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
        "\t\treturn (cosine_similarity(X, Y) + 1.0) / 2.0\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef average_embeds_over_words(bpe_vectors: np.ndarray, word_tokens_pair: List[List[str]]) -> List[np.array]:\n",
        "\t\tw2b_map = []\n",
        "\t\tcnt = 0\n",
        "\t\tw2b_map.append([])\n",
        "\t\tfor wlist in word_tokens_pair[0]:\n",
        "\t\t\tw2b_map[0].append([])\n",
        "\t\t\tfor x in wlist:\n",
        "\t\t\t\tw2b_map[0][-1].append(cnt)\n",
        "\t\t\t\tcnt += 1\n",
        "\t\tcnt = 0\n",
        "\t\tw2b_map.append([])\n",
        "\t\tfor wlist in word_tokens_pair[1]:\n",
        "\t\t\tw2b_map[1].append([])\n",
        "\t\t\tfor x in wlist:\n",
        "\t\t\t\tw2b_map[1][-1].append(cnt)\n",
        "\t\t\t\tcnt += 1\n",
        "\n",
        "\t\tnew_vectors = []\n",
        "\t\tfor l_id in range(2):\n",
        "\t\t\tw_vector = []\n",
        "\t\t\tfor word_set in w2b_map[l_id]:\n",
        "\t\t\t\tw_vector.append(bpe_vectors[l_id][word_set].mean(0))\n",
        "\t\t\tnew_vectors.append(np.array(w_vector))\n",
        "\t\treturn new_vectors\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef get_alignment_matrix(sim_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "\t\tm, n = sim_matrix.shape\n",
        "\t\tforward = np.eye(n)[sim_matrix.argmax(axis=1)]  # m x n\n",
        "\t\tbackward = np.eye(m)[sim_matrix.argmax(axis=0)]  # n x m\n",
        "\t\treturn forward, backward.transpose()\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef apply_distortion(sim_matrix: np.ndarray, ratio: float = 0.5) -> np.ndarray:\n",
        "\t\tshape = sim_matrix.shape\n",
        "\t\tif (shape[0] < 2 or shape[1] < 2) or ratio == 0.0:\n",
        "\t\t\treturn sim_matrix\n",
        "\n",
        "\t\tpos_x = np.array([[y / float(shape[1] - 1) for y in range(shape[1])] for x in range(shape[0])])\n",
        "\t\tpos_y = np.array([[x / float(shape[0] - 1) for x in range(shape[0])] for y in range(shape[1])])\n",
        "\t\tdistortion_mask = 1.0 - ((pos_x - np.transpose(pos_y)) ** 2) * ratio\n",
        "\n",
        "\t\treturn np.multiply(sim_matrix, distortion_mask)\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef iter_max(sim_matrix: np.ndarray, max_count: int=2) -> np.ndarray:\n",
        "\t\talpha_ratio = 0.9\n",
        "\t\tm, n = sim_matrix.shape\n",
        "\t\tforward = np.eye(n)[sim_matrix.argmax(axis=1)]  # m x n\n",
        "\t\tbackward = np.eye(m)[sim_matrix.argmax(axis=0)]  # n x m\n",
        "\t\tinter = forward * backward.transpose()\n",
        "\n",
        "\t\tif min(m, n) <= 2:\n",
        "\t\t\treturn inter\n",
        "\n",
        "\t\tnew_inter = np.zeros((m, n))\n",
        "\t\tcount = 1\n",
        "\t\twhile count < max_count:\n",
        "\t\t\tmask_x = 1.0 - np.tile(inter.sum(1)[:, np.newaxis], (1, n)).clip(0.0, 1.0)\n",
        "\t\t\tmask_y = 1.0 - np.tile(inter.sum(0)[np.newaxis, :], (m, 1)).clip(0.0, 1.0)\n",
        "\t\t\tmask = ((alpha_ratio * mask_x) + (alpha_ratio * mask_y)).clip(0.0, 1.0)\n",
        "\t\t\tmask_zeros = 1.0 - ((1.0 - mask_x) * (1.0 - mask_y))\n",
        "\t\t\tif mask_x.sum() < 1.0 or mask_y.sum() < 1.0:\n",
        "\t\t\t\tmask *= 0.0\n",
        "\t\t\t\tmask_zeros *= 0.0\n",
        "\n",
        "\t\t\tnew_sim = sim_matrix * mask\n",
        "\t\t\tfwd = np.eye(n)[new_sim.argmax(axis=1)] * mask_zeros\n",
        "\t\t\tbac = np.eye(m)[new_sim.argmax(axis=0)].transpose() * mask_zeros\n",
        "\t\t\tnew_inter = fwd * bac\n",
        "\n",
        "\t\t\tif np.array_equal(inter + new_inter, inter):\n",
        "\t\t\t\tbreak\n",
        "\t\t\tinter = inter + new_inter\n",
        "\t\t\tcount += 1\n",
        "\t\treturn inter\n",
        "\n",
        "\tdef get_word_aligns(self, src_sent: Union[str, List[str]], trg_sent: Union[str, List[str]]) -> Dict[str, List]:\n",
        "\t\tif isinstance(src_sent, str):\n",
        "\t\t\tsrc_sent = src_sent.split()\n",
        "\t\tif isinstance(trg_sent, str):\n",
        "\t\t\ttrg_sent = trg_sent.split()\n",
        "\t\tl1_tokens = [self.embed_loader.tokenizer.tokenize(word) for word in src_sent]\n",
        "\t\tl2_tokens = [self.embed_loader.tokenizer.tokenize(word) for word in trg_sent]\n",
        "\t\tbpe_lists = [[bpe for w in sent for bpe in w] for sent in [l1_tokens, l2_tokens]]\n",
        "\n",
        "\t\tif self.token_type == \"bpe\":\n",
        "\t\t\tl1_b2w_map = []\n",
        "\t\t\tfor i, wlist in enumerate(l1_tokens):\n",
        "\t\t\t\tl1_b2w_map += [i for x in wlist]\n",
        "\t\t\tl2_b2w_map = []\n",
        "\t\t\tfor i, wlist in enumerate(l2_tokens):\n",
        "\t\t\t\tl2_b2w_map += [i for x in wlist]\n",
        "\n",
        "\t\tvectors = self.embed_loader.get_embed_list([src_sent, trg_sent]).cpu().detach().numpy()\n",
        "\t\tvectors = [vectors[i, :len(bpe_lists[i])] for i in [0, 1]]\n",
        "\n",
        "\t\tif self.token_type == \"word\":\n",
        "\t\t\tvectors = self.average_embeds_over_words(vectors, [l1_tokens, l2_tokens])\n",
        "\n",
        "\t\tall_mats = {}\n",
        "\t\tsim = self.get_similarity(vectors[0], vectors[1])\n",
        "\t\tsim = self.apply_distortion(sim, self.distortion)\n",
        "\n",
        "\t\tall_mats[\"fwd\"], all_mats[\"rev\"] = self.get_alignment_matrix(sim)\n",
        "\t\tall_mats[\"inter\"] = all_mats[\"fwd\"] * all_mats[\"rev\"]\n",
        "\t\tif \"mwmf\" in self.matching_methods:\n",
        "\t\t\tall_mats[\"mwmf\"] = self.get_max_weight_match(sim)\n",
        "\t\tif \"itermax\" in self.matching_methods:\n",
        "\t\t\tall_mats[\"itermax\"] = self.iter_max(sim)\n",
        "\n",
        "\t\taligns = {x: set() for x in self.matching_methods}\n",
        "\t\tfor i in range(len(vectors[0])):\n",
        "\t\t\tfor j in range(len(vectors[1])):\n",
        "\t\t\t\tfor ext in self.matching_methods:\n",
        "\t\t\t\t\tif all_mats[ext][i, j] > 0:\n",
        "\t\t\t\t\t\tif self.token_type == \"bpe\":\n",
        "\t\t\t\t\t\t\taligns[ext].add((l1_b2w_map[i], l2_b2w_map[j]))\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\taligns[ext].add((i, j))\n",
        "\t\tfor ext in aligns:\n",
        "\t\t\taligns[ext] = sorted(aligns[ext])\n",
        "\t\treturn aligns"
      ],
      "metadata": {
        "id": "9BctbyMPuLTS"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making an instance of our model.\n",
        "# You can specify the embedding model and all alignment settings in the constructor.\n",
        "myaligner = SentenceAligner(model=\"xlm-roberta-base\", token_type=\"bpe\", matching_methods=\"mai\") \n",
        "#myaligner = SentenceAligner(model=\"bert-base-multilingual-uncased\", token_type=\"bpe\", matching_methods=\"mai\") \n",
        "#myaligner = SentenceAligner(model=\"xlm-mlm-100-1280\", token_type=\"bpe\", matching_methods=\"mai\") \n",
        "#myaligner = SentenceAligner(model=\"roberta-base\", token_type=\"bpe\", matching_methods=\"mai\") \n",
        "#myaligner = SentenceAligner(model=\"xlm-roberta-large\", token_type=\"bpe\", matching_methods=\"mai\")\n",
        "#myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxWLjaNouLrh",
        "outputId": "50eb0a4f-a258-4ebb-cf60-81057a8ab9d8"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2022-08-03 15:20:58,401 - __main__ - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
            "2022-08-03 15:20:58,401 - __main__ - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
            "2022-08-03 15:20:58,401 - __main__ - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
            "2022-08-03 15:20:58,401 - __main__ - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
            "2022-08-03 15:20:58,401 - __main__ - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
            "2022-08-03 15:20:58,401 - __main__ - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
            "2022-08-03 15:20:58,401 - __main__ - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
            "INFO:__main__:Initialized the EmbeddingLoader with model: xlm-roberta-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The source and target sentences should be tokenized to words.\n",
        "#Experiment - 1 \n",
        "src_sentence = [\"varsh\",\"1985\",\"86\",\"men\",\"tatkalin\",\"kalyan\",\"ministry\",\"ko\",\"mahila\",\"and\",\"bal\",\"development\"] \n",
        "trg_sentence = [\"in\",\"the\",\"varsh\",\"1985\",\"86\",\"the\",\"erstwhile\",\"mntralay\",\"of\"]\n",
        "\n",
        "#Experiment - 2\n",
        "#src_sentence = [\"and\",\"islie\",\"ke\",\"roop\",\"men\",\"in\",\"tatvon\",\"come\",\"ek\",\"sath\"]\n",
        "#trg_sentence = [\"aur\",\"slie\",\"as\",\"in\",\"elements\",\"come\",\"ek\",\"sath\"]"
      ],
      "metadata": {
        "id": "qvxtJXdvuZMR"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# src_sentence = []\n",
        "# trg_sentence = []\n",
        "# for word in src_sentence1:\n",
        "#   if word not in stop_words:\n",
        "#     src_sentence.append(word)\n",
        "\n",
        "# for word in trg_sentence1:\n",
        "#   if word not in stop_words:\n",
        "#     trg_sentence.append(word)\n",
        "\n",
        "#print([x for x in src_sentence])\n",
        "#print([x for x in src_sentence1])\n",
        "#print([x for x in trg_sentence])\n",
        "#print([x for x in trg_sentence1])"
      ],
      "metadata": {
        "id": "_Fe265BjPwuI",
        "outputId": "d758274d-82d4-447c-ab27-d7c95c0ab7ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The output is a dictionary with different matching methods.\n",
        "# Each method has a list of pairs indicating the indexes of aligned words (The alignments are zero-indexed).\n",
        "alignments = myaligner.get_word_aligns(src_sentence, trg_sentence)\n",
        "\n",
        "for i in list(enumerate(src_sentence)):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRbOMVbnwlH7",
        "outputId": "a2c382ed-e332-4893-aa79-e4a6ec508c4d"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'and')\n",
            "(1, 'islie')\n",
            "(2, 'ke')\n",
            "(3, 'roop')\n",
            "(4, 'men')\n",
            "(5, 'in')\n",
            "(6, 'tatvon')\n",
            "(7, 'come')\n",
            "(8, 'ek')\n",
            "(9, 'sath')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(enumerate(trg_sentence)):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nukld1f6zRP2",
        "outputId": "0b595150-701e-41bb-e27a-fcde84eac177"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'aur')\n",
            "(1, 'slie')\n",
            "(2, 'as')\n",
            "(3, 'in')\n",
            "(4, 'elements')\n",
            "(5, 'come')\n",
            "(6, 'ek')\n",
            "(7, 'sath')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for matching_method in alignments:\n",
        "    print(matching_method, \":\", alignments[matching_method])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30Dq7cjywoWk",
        "outputId": "53ac704d-922b-4e87-f160-5fcfb0175bc3"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mwmf : [(0, 0), (1, 2), (3, 1), (4, 4), (5, 3), (7, 5), (8, 6), (9, 7)]\n",
            "inter : [(0, 0), (3, 1), (4, 4), (5, 3), (7, 5), (8, 6), (9, 7)]\n",
            "itermax : [(0, 0), (1, 2), (3, 1), (4, 4), (5, 3), (7, 5), (8, 6), (9, 7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Possible Alignments From SimAlign\")\n",
        "print(\"Word in Sent 1 -----> Word in Sent 2\")\n",
        "sent1 = []\n",
        "sent2 = []\n",
        "for item in alignments['itermax']:\n",
        "  print(src_sentence[item[0]],\"---------->\",trg_sentence[item[1]])\n",
        "  sent1.append(src_sentence[item[0]])\n",
        "  sent2.append(trg_sentence[item[1]])"
      ],
      "metadata": {
        "id": "C6CaXwgmzXPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f554d88-93da-431b-d350-5cc2d50b79b6"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible Alignments From SimAlign\n",
            "Word in Sent 1 -----> Word in Sent 2\n",
            "and ----------> aur\n",
            "islie ----------> as\n",
            "roop ----------> slie\n",
            "men ----------> elements\n",
            "in ----------> in\n",
            "come ----------> come\n",
            "ek ----------> ek\n",
            "sath ----------> sath\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_align_pairs = zip(sent1,sent2)"
      ],
      "metadata": {
        "id": "M6Mux7fx0a1V"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for x,y in word_align_pairs:\n",
        "#   print(x,y)"
      ],
      "metadata": {
        "id": "EPQtKFWutgzI"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine Similarity"
      ],
      "metadata": {
        "id": "TXr_pTOnsiLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "XAVBvZZGraZ1"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "yIiku7wsrhtA"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model\n",
        "model = SentenceTransformer('sentence-transformers/msmarco-distilbert-cos-v5')"
      ],
      "metadata": {
        "id": "6a02LpCpsso5"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_score = []\n",
        "for query, docs_t in word_align_pairs:\n",
        "  #Encode query and documents\n",
        "  docs = []\n",
        "  docs.append(docs_t)\n",
        "  query_emb = model.encode(query)\n",
        "  doc_emb = model.encode(docs)\n",
        "\n",
        "  #Compute dot score between query and all document embeddings\n",
        "  scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
        "  \n",
        "  #Combine docs & scores\n",
        "  doc_score_pairs = list(zip(docs, scores))\n",
        "\n",
        "  #Sort by decreasing score\n",
        "  doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  #Output passages & scores\n",
        "  for doc, score in doc_score_pairs:\n",
        "    print(score, doc)\n",
        "    list_score.append(score)\n",
        "all_stats = zip(sent1,sent2,list_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqTNhuRJtacM",
        "outputId": "7d032715-70a0-4400-9635-7f7e3777ddab"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.214100643992424 aur\n",
            "0.1816745102405548 as\n",
            "0.0595242939889431 slie\n",
            "0.14680269360542297 elements\n",
            "0.9999997615814209 in\n",
            "0.9999998807907104 come\n",
            "1.0 ek\n",
            "1.0 sath\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word1,word2,score in all_stats:\n",
        "  #print(\"[\",word1,\"---->\",word2,\"] [cosine similarity: \",round(score, 2),\"]\")\n",
        "  if word1 not in stop_words and word2 not in stop_words and word1.lower() != word2.lower() and round(score,2) > 0.50:\n",
        "    print(\"[\",word1,\"---->\",word2,\"] [cosine similarity: \",round(score, 2),\"]\")\n",
        "    "
      ],
      "metadata": {
        "id": "7qDCKWZCukKC"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "23nfByce0FYh"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wordalignment using cosine similarity "
      ],
      "metadata": {
        "id": "9HZvsEEgiAuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "-V03qNk8iHon"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model\n",
        "model = SentenceTransformer('sentence-transformers/msmarco-distilbert-cos-v5')"
      ],
      "metadata": {
        "id": "qwIHASIikXS2"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "LiXjA-9VkZsR"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open(\"Microsoft_without_GA_858.txt\",\"r\") as fp:\n",
        "  data = fp.read().split('\\n')"
      ],
      "metadata": {
        "id": "R1oRLDpWkdvH"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"\"\n",
        "trg_sentence = []\n",
        "pop = []\n",
        "def group_parallel_sent_pair():\n",
        "  parallel_sent_pair = []\n",
        "  l = []\n",
        "  for i in range(len(data)):\n",
        "    if data[i] == '':\n",
        "      pop.append(l)\n",
        "      l = []\n",
        "      i = i + 1\n",
        "      continue\n",
        "    l.append(data[i])\n",
        "group_parallel_sent_pair()"
      ],
      "metadata": {
        "id": "E9LQFIX3mfpn"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_1 = pop[0][0]\n",
        "sent_2 = pop[0][1]"
      ],
      "metadata": {
        "id": "rzieXosanFE_"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "Mi02tXOHoUaG"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = word_tokenize(sent_1)\n",
        "sent2 = word_tokenize(sent_2)"
      ],
      "metadata": {
        "id": "ZlkD_38iqcmi"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop words\n",
        "#import nltk\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "7suIKQDvrFg5"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1_without_stopwords = [word for word in sent1 if word not in stop_words ]\n",
        "sent2_without_stopwords = [word for word in sent2 if word not in stop_words ]"
      ],
      "metadata": {
        "id": "hWfXTzq0rsfC"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_sent1 = [unique_word for unique_word in sent1_without_stopwords if unique_word not in sent2_without_stopwords]\n",
        "final_sent2 = [unique_word for unique_word in sent2_without_stopwords if unique_word not in sent1_without_stopwords]"
      ],
      "metadata": {
        "id": "_1XAvv7csSss"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_align_pairs = zip(final_sent1,final_sent2)"
      ],
      "metadata": {
        "id": "kkvXRNjssmKX"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_word_pair = {}\n",
        "key = 0"
      ],
      "metadata": {
        "id": "HpWUQW1gxjPN"
      },
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word1 in final_sent1:\n",
        "  for word2 in final_sent2:\n",
        "    #Encode query and documents\n",
        "     docs = []\n",
        "     docs.append(word2)\n",
        "     query_emb = model.encode(word1)\n",
        "     doc_emb = model.encode(docs)\n",
        "\n",
        "     #Compute dot score between query and all document embeddings\n",
        "     scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
        "\n",
        "     #Combine docs & scores\n",
        "     doc_score_pairs = list(zip(docs, scores))\n",
        "\n",
        "     #Sort by decreasing score\n",
        "     doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "     #Output passages & scores\n",
        "     for doc, score in doc_score_pairs:\n",
        "       #print(score, doc)\n",
        "       dict_word_pair[key] = str(\"[\"+word1 + \"--->\" + word2 + \"] [score: \"+ str(score)+\"]\")\n",
        "       key+=1"
      ],
      "metadata": {
        "id": "-1GegtQDwlSc"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dict_word_pair.values():\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "rcM920fgy2Cs",
        "outputId": "18d56286-507e-483f-8289-6bd3d55f439c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[varsh--->year] [score: 0.06793226301670074]\n",
            "[varsh--->erstwhile] [score: 0.06392823904752731]\n",
            "[varsh--->aur] [score: 0.152662992477417]\n",
            "[varsh--->child] [score: -0.039615266025066376]\n",
            "[varsh--->vikas] [score: 0.3038002848625183]\n",
            "[tatkalin--->year] [score: -0.004085337743163109]\n",
            "[tatkalin--->erstwhile] [score: -0.08381135016679764]\n",
            "[tatkalin--->aur] [score: 0.021452924236655235]\n",
            "[tatkalin--->child] [score: -0.10026955604553223]\n",
            "[tatkalin--->vikas] [score: 0.058305129408836365]\n",
            "[bal--->year] [score: -0.007758052088320255]\n",
            "[bal--->erstwhile] [score: 0.0379321351647377]\n",
            "[bal--->aur] [score: 0.22376605868339539]\n",
            "[bal--->child] [score: 0.03532685711979866]\n",
            "[bal--->vikas] [score: 0.18184608221054077]\n",
            "[development--->year] [score: 0.0746079608798027]\n",
            "[development--->erstwhile] [score: -0.020982349291443825]\n",
            "[development--->aur] [score: -0.047673553228378296]\n",
            "[development--->child] [score: 0.21531352400779724]\n",
            "[development--->vikas] [score: 0.10878893733024597]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key"
      ],
      "metadata": {
        "id": "asS0lBGO0EJC",
        "outputId": "a6762c6d-7a15-4830-e56a-201fbe700c3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_score = []\n",
        "for query, docs_t in word_align_pairs:\n",
        "  #Encode query and documents\n",
        "  docs = []\n",
        "  docs.append(docs_t)\n",
        "  query_emb = model.encode(query)\n",
        "  doc_emb = model.encode(docs)\n",
        "\n",
        "  #Compute dot score between query and all document embeddings\n",
        "  scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
        "  \n",
        "  #Combine docs & scores\n",
        "  doc_score_pairs = list(zip(docs, scores))\n",
        "\n",
        "  #Sort by decreasing score\n",
        "  doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  #Output passages & scores\n",
        "  for doc, score in doc_score_pairs:\n",
        "    print(score, doc)\n",
        "    list_score.append(score)\n",
        "all_stats = zip(final_sent1,final_sent2,list_score)"
      ],
      "metadata": {
        "id": "tOamfOjbsoK7",
        "outputId": "83346996-bf8f-4ed1-9d98-1bae1cde7ffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06793226301670074 year\n",
            "-0.08381135016679764 erstwhile\n",
            "0.22376605868339539 aur\n",
            "0.21531352400779724 child\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word1,word2,score in all_stats:\n",
        "  #print(\"[\",word1,\"---->\",word2,\"] [cosine similarity: \",round(score, 2),\"]\")\n",
        "  if word1 not in stop_words and word2 not in stop_words and word1.lower() != word2.lower() and round(score,2) > 0.50:\n",
        "    print(\"[\",word1,\"---->\",word2,\"] [cosine similarity: \",round(score, 2),\"]\")"
      ],
      "metadata": {
        "id": "UvXWKeuEukNR"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_sent1"
      ],
      "metadata": {
        "id": "s9XF0v2Xumaa",
        "outputId": "2534691a-ddae-4e57-d5fd-45b169b0a5cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['varsh', 'tatkalin', 'bal', 'development']"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_sent2"
      ],
      "metadata": {
        "id": "SUILJ1NYvtv8",
        "outputId": "f03fcfa7-86a6-4482-d87e-6b45e7b12b8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['year', 'erstwhile', 'aur', 'child', 'vikas']"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XGGag6Q9vvuh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}